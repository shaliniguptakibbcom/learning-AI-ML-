{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End Project - Bikes Assessment\n",
    "\n",
    "Task: The dataset (Location: /cxldata/datasets/project/bikes.csv) contains the hourly rental bike demand data. The goal is to develop a model to estimate the bike demand in future given the parameters as observed in the past.\n",
    "\n",
    "We will be following this example step-by-step in this assessment:\n",
    "1. Importing the libraries\n",
    "1. Defining some utility functions\n",
    "1. Loading the data\n",
    "1. Cleaning the data\n",
    "1. Adding derived features\n",
    "1. Analyzing the dataset\n",
    "1. Dividing the dataset into training and test dataset\n",
    "1. Training several models and analyzing their performance\n",
    "1. Selecting a model and evaluating using test dataset\n",
    "1. Improving the model by finding the best hyper-parameters and features\n",
    "1. Analyzing the residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries\n",
    "We will import these libraries into the environment\n",
    "\n",
    "1. **numpy:** np\n",
    "1. **pandas:** pd\n",
    "1. **sklearn** - preprocessing, linear_model, StandardScaler, mean_squared_error\n",
    "1. **matplotplib.pyplot:** plt\n",
    "1. **os**\n",
    "\n",
    "Please complete the code to import these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Task: import os\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_day(df):\n",
    "    '''\n",
    "    This function assigns day names to each of the\n",
    "    rows in the dataset. \n",
    "    '''\n",
    "    ## Assumes the first day of the dataset is Saturday\n",
    "    days = [\"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thr\", \"Fri\"]\n",
    "    temp = ['d']*df.shape[0]\n",
    "    i = 0\n",
    "    indx = 0\n",
    "    cur_day = df.weekday[0]\n",
    "    for day in df.weekday:\n",
    "        temp[indx] = days[(day-cur_day+7)%7]\n",
    "        indx += 1\n",
    "    df['dayWeek'] = temp\n",
    "    return df\n",
    "\n",
    "# Function that takes in a dataframe with yr and mnth attribute and calculates an array denoting the month number from the start\n",
    "def mnth_cnt(df):\n",
    "    '''\n",
    "    Compute the count of months from the start of\n",
    "    the time series.\n",
    "    '''\n",
    "    import itertools\n",
    "    yr = df['yr'].tolist()\n",
    "    mnth = df['mnth'].tolist()\n",
    "    out = [0] * df.shape[0]\n",
    "    indx = 0\n",
    "    for x, y in zip(mnth, yr):\n",
    "        out[indx] = x + 12 * y\n",
    "        indx += 1\n",
    "    return out\n",
    "\n",
    "\n",
    "# Function used to calculate the basics stats of observed scores from cross-validation of models\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "The dataset can be loaded from a csv which is present on the shared drive at the location: \n",
    "(Location: /cxldata/datasets/project/bikes.csv)\n",
    "\n",
    "The dataset contains the following parameters:\n",
    "* instant: record index\n",
    "* dteday : date\n",
    "* season : season (1:springer, 2:summer, 3:fall, 4:winter)\n",
    "* yr : year (0: 2011, 1:2012)\n",
    "* mnth : month ( 1 to 12)\n",
    "* hr : hour (0 to 23)\n",
    "* holiday : weather day is holiday or not (extracted from [Web Link])\n",
    "* weekday : day of the week\n",
    "* workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "+ weathersit : \n",
    "    * 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    * 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    * 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    * 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "* temp : Normalized temperature in Celsius. The values are derived via (t*t_min)/(t_max*t_min), t_min=*8, t_max=+39 (only in hourly scale)\n",
    "* atemp: Normalized feeling temperature in Celsius. The values are derived via (t*t_min)/(t_max*t_min), t_min=*16, t_max=+50 (only in hourly scale)\n",
    "* hum: Normalized humidity. The values are divided to 100 (max)\n",
    "* windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "* casual: count of casual users\n",
    "* registered: count of registered users\n",
    "* cnt: count of total rental bikes including both casual and registered\n",
    "\n",
    "**Task:** Please complete the read statement correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filePath = '/cxldata/datasets/project/bikes.csv'\n",
    "\n",
    "## Task: Complete the statement below\n",
    "bikesData = pd.read_csv(filePath)\n",
    "\n",
    "print(bikesData.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(bikesData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "As we observe, some of the attributes are not required as per the requirement of the project: ['instant','casual','registered','atemp','dteday']. These can be dropped.\n",
    "\n",
    "Some of the numerical columns will have to be scaled: ['temp','hum','windspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 1: Update the columnsToDrop variable in the order given\n",
    "columnsToDrop = ['instant','casual','registered','atemp','dteday']\n",
    "\n",
    "# Task 2\n",
    "bikesData = bikesData.drop(columnsToDrop,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 3: Update the columnsToScale variable in the order given\n",
    "columnsToScale = ['temp','hum','windspeed']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Task 4\n",
    "bikesData[columnsToScale] = scaler.fit_transform(bikesData[columnsToScale])\n",
    "bikesData[columnsToScale].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding derived features and transforming the data\n",
    "These features are derived from the raw set of features:\n",
    "1. **isWorking:** 1: Is a workingday and not a holiday, 0: Is not a workingday and is a holiday\n",
    "1. **monthCount:** count of the number of months from the beginning of the dataset\n",
    "1. **xformHr:** transform by shifting the hours by 5 hrs, if the hours are greater than 5, we subtract 5, else we add 19.\n",
    "1. **dayCnt:** count of the days from the beginning of the dataset\n",
    "1. **xformWorkHr:** transforming the hour dataset to make the non-working days to have hours from 25 to 48\n",
    "1. **cntDeTrended:** De-trended count values\n",
    "\n",
    "**Task:** Complete the statements for calculating each of the above derived attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "bikesData['isWorking'] = np.where(np.logical_and(bikesData.workingday==1,bikesData.holiday==0),1,0)\n",
    "bikesData['monthCount'] = mnth_cnt(bikesData)\n",
    "bikesData['xformHr'] = np.where(bikesData.hr>4,bikesData.hr-5,bikesData.hr+19)\n",
    "bikesData['dayCount'] = pd.Series(range(bikesData.shape[0]))/24\n",
    "bikesData['xformWorkHr'] = (1-bikesData.isWorking)*24 + bikesData.xformHr\n",
    "bikesData = set_day(bikesData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bikesData.sort_values('dayCount', axis= 0, inplace=True)\n",
    "nrow = bikesData.shape[0]\n",
    "X = bikesData.dayCount.values.reshape(nrow,1)\n",
    "Y = bikesData.cnt\n",
    "clf = linear_model.LinearRegression()\n",
    "bike_lm = clf.fit(X, Y)\n",
    "\n",
    "# Task\n",
    "bikesData['cntDeTrended'] = bikesData.cnt - bike_lm.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing and visualizing the dataset\n",
    "\n",
    "These tasks shall be performed as a part of analyzing and visualizing the dataset:\n",
    "1. Hourly count of bikes with trend and without trend\n",
    "1. Correlation matrix between the features and the dependent variable\n",
    "1. Plotting correlation among selected variables - 'yr','mnth','isWorking','xformWorkHr','dayCount','temp','hum','windspeed','cntDeTrended'\n",
    "1. Plotting the count pattern with daycount for specific hours\n",
    "1. Plotting box plot for different attributes: 'hr', 'mnth', 'weathersit', 'isWorking', 'dayWeek', 'xformHr'\n",
    "1. Plotting scatter matrix for selected attributes: 'temp', 'hum', 'windspeed', 'hr', 'xformHr', 'cntDeTrended'\n",
    "1. Plotting box plots for 0900 and 1800 hrs working and non-working days\n",
    "1. Plotting the demand counts for each of the transformed work hours which include working and non-working hours\n",
    "\n",
    "**Task:** Complete the statement to complete plotting the hourly count with trend and without trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Task: Complete the statement to complete plotting the hourly count with trend and without trend.\n",
    "plt.plot(bikesData.loc[:,'cnt'])\n",
    "plt.plot(bikesData.loc[:,'cntDeTrended'])\n",
    "plt.plot(bike_lm.predict(X))\n",
    "plt.legend(['With trend','Detrended','Trend'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ignore\n",
    "#bikesData.info()\n",
    "corrMat = bikesData.corr().as_matrix()\n",
    "np.fill_diagonal(corrMat, 0)\n",
    "plt.matshow(corrMat)\n",
    "plt.xlabel(bikesData.columns.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix\n",
    "**Task 1:** Complete the statement to plot the correlation matrix between all the features and the dependent variable.\n",
    "\n",
    "**Task 2:** Complete the statement to calculate correlation among these variables: 'yr', 'mnth', 'isWorking', 'xformWorkHr', 'dayCount', 'temp', 'hum', 'windspeed', 'cntDeTrended'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 1: Is there anything missing here?\n",
    "import statsmodels.graphics.correlation as pltcor\n",
    "arr = bikesData.drop('dayWeek', axis = 1)\n",
    "cols = list(arr)\n",
    "arr = arr.as_matrix()\n",
    "arr = preprocessing.scale(arr, axis = 1)\n",
    "corrMat = np.corrcoef(arr, rowvar =0)\n",
    "np.fill_diagonal(corrMat, 0)\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.gca()\n",
    "pltcor.plot_corr(corrMat, xnames = cols, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ignore\n",
    "columnToPlotScatter = ['yr','mnth','isWorking','xformWorkHr','dayCount','temp','hum','windspeed','cntDeTrended']\n",
    "corrMat = bikesData[columnToPlotScatter].corr().as_matrix()\n",
    "np.fill_diagonal(corrMat, 0)\n",
    "plt.matshow(corrMat)\n",
    "plt.xlabel(bikesData[columnToPlotScatter].columns.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Task 2\n",
    "columnToPlotScatter = ['yr','mnth','isWorking','xformWorkHr','dayCount','temp','hum','windspeed','cntDeTrended']\n",
    "\n",
    "arry = bikesData[columnToPlotScatter].as_matrix()\n",
    "arry = preprocessing.scale(arry, axis = 1)\n",
    "corrs = np.corrcoef(arry, rowvar = 0)\n",
    "np.fill_diagonal(corrs, 0)\n",
    "col_nms = list(bikesData)[1:]\n",
    "fig = plt.figure(figsize = (7,7))\n",
    "ax = fig.gca()\n",
    "pltcor.plot_corr(corrs, xnames = columnToPlotScatter, ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line Plots\n",
    "\n",
    "**Task 1:** Complete the code to plot cntDeTrended versus dayCount for these hours: [7, 9, 12, 15, 18, 20, 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task 1: Complete the times variable\n",
    "times = [7, 9, 12, 15, 18, 20, 22]\n",
    "for time in times:\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    tsToPlot = bikesData[bikesData.hr==time]\n",
    "    fig.clf()\n",
    "    ax = fig.gca()\n",
    "    tsToPlot.plot(kind='line', x='dayCount', y='cntDeTrended', ax =ax)\n",
    "    plt.xlabel(\"Days from start of plot\")\n",
    "    plt.ylabel(\"Count of bikes rented\")\n",
    "    plt.title(\"Bikes rented at hour \" + str(time))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box plots\n",
    "\n",
    "**Task:** Complete the statement to include these box plots and in this order:\n",
    "['hr', 'mnth', 'weathersit', 'isWorking', 'dayWeek', 'xformHr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task: Complete the statement to include these box plots and in this order: ['hr', 'mnth', 'weathersit', 'isWorking', 'dayWeek', 'xformHr']\n",
    "colstoBoxPlot = ['hr','mnth','weathersit','isWorking','dayWeek','xformHr']\n",
    "\n",
    "for cols in colstoBoxPlot:\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    fig.clf()\n",
    "    ax = fig.gca()\n",
    "    bikesData.boxplot(column=['cntDeTrended'], by = [cols], ax = ax)\n",
    "    plt.xlabel('Box Plot of bike detrended counts by '+str(cols))\n",
    "    plt.ylabel('Number of bikes')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Matrix\n",
    "\n",
    "**Task:** Complete the statements to plot the scatter matrix for these columns: ['temp', 'hum', 'windspeed', 'hr', 'xformHr', 'cntDeTrended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task: Complete the statements to plot the scatter matrix for these columns: ['temp', 'hum', 'windspeed', 'hr', 'xformHr', 'cntDeTrended']\n",
    "columnToPlotScatter = ['temp','hum','windspeed','hr','xformHr','cntDeTrended']\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(bikesData[columnToPlotScatter], figsize=(12,8), alpha=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting box plots for 0900 and 1800 hrs working and non-working days\n",
    "\n",
    "Contrary to the previous observation. there is perhaps an interaction between the time of day of working and nonworking days. A day of week effect is not apparent, but we may need to look in more detail. This idea is easy to explore. \n",
    "\n",
    "**Task:** Adding the following code creates box plots for peak demand hours of working and nonworking days. Complete the code to box plot the bike counts for isWorking - [0,1] for these hours: [0900, 1800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [\"Boxplots of bike demand at 0900 \\n\\n\", \"Boxplots of bike demand at 1800 \\n\\n\"]\n",
    "times = [8, 17]\n",
    "# Task: Complete these statements\n",
    "col = ['cntDeTrended']\n",
    "byCol = ['isWorking']\n",
    "for lab, tms in zip(labels, times):\n",
    "    temp = bikesData[bikesData.hr == tms]\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    fig.clf()\n",
    "    ax = fig.gca()\n",
    "    \n",
    "    temp.boxplot(column = col, by = byCol, ax = ax)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Number of bikes')\n",
    "    plt.title(lab)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Transformed Work hours versus the count\n",
    "\n",
    "In accordance with our previous observation, we shall plot the demand counts for each of the transformed work hours which includes working and non-working hours.\n",
    "\n",
    "**Task**: Complete the statement to plot detrended count and the xformWorkHr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task\n",
    "x_plot = 'xformWorkHr'\n",
    "y_plot = 'cntDeTrended'\n",
    "ax = fig.gca()\n",
    "bikesData.plot(kind='scatter', x = x_plot, y = y_plot, alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the dataset into training and test dataset\n",
    "\n",
    "After having analyzed the dataset, we shall divide the entire dataset into training and test set using train_test_split in the ratio 70:30 It uses random sorting and hence the resulting train_set and test_set is sorted by daycount.\n",
    "\n",
    "**Task:** Correct the train_test_split function to split the test set in the ratio 70:30.\n",
    "\t\tfrom sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Task: Correct the train_test_split function to split the test set in the ratio 70:30\n",
    "train_set, test_set = train_test_split(bikesData, test_size=0.3, random_state=42)\n",
    "train_set.sort_values('dayCount', axis= 0, inplace=True)\n",
    "test_set.sort_values('dayCount', axis= 0, inplace=True)\n",
    "print(len(train_set), \"train +\", len(test_set), \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and analyze models\n",
    "\n",
    "Models to be trained and analyzed: \n",
    "1. DecisionTreeRegressor\n",
    "1. LinearRegression \n",
    "1. RandomForestRegressor\n",
    "\n",
    "Metrics calculated: neg_mean_absolute_error, neg_mean_squared_error using cross-validation\n",
    "\n",
    "**Task 1:** Complete the statement to define forest_reg as a RandomForestRegressor with random_state = 42\n",
    "\n",
    "**Task 2:** Store predicted values from the classifier using cross_val_predict. As identified as action tasks Consider 'xformHr', 'xformWorkHr','temp' as the training features and 10 folds.\n",
    "\n",
    "Features used:\n",
    "1. xformWorkHr\n",
    "1. temp\n",
    "1. dayCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingCols = train_set[['xformWorkHr','temp','dayCount']]\n",
    "trainingLabels = train_set['cnt']\n",
    "\n",
    "# Train a Decision Tree Regressor\n",
    "rfc_clf = DecisionTreeRegressor(random_state = 42)\n",
    "display_scores(-cross_val_score(rfc_clf, trainingCols, trainingLabels, cv=10, scoring=\"neg_mean_absolute_error\"))\n",
    "display_scores(np.sqrt(-cross_val_score(rfc_clf, trainingCols, trainingLabels, cv=10, scoring=\"neg_mean_squared_error\")))\n",
    "train_set_dtr = train_set.copy()\n",
    "train_set_dtr['predictedCounts'] = cross_val_predict(rfc_clf, trainingCols, trainingLabels, cv=10)\n",
    "train_set_dtr['resids'] = train_set_dtr['predictedCounts'] - trainingLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a Linear Regression model\n",
    "lin_reg = LinearRegression()\n",
    "display_scores(-cross_val_score(lin_reg, trainingCols, trainingLabels, cv=10, scoring=\"neg_mean_absolute_error\"))\n",
    "display_scores(np.sqrt(-cross_val_score(lin_reg, trainingCols, trainingLabels, cv=10, scoring=\"neg_mean_squared_error\")))\n",
    "train_set_lin = train_set.copy()\n",
    "train_set_lin['predictedCounts'] = cross_val_predict(rfc_clf, trainingCols, trainingLabels, cv=10)\n",
    "train_set_lin['resids'] = train_set_lin['predictedCounts'] - trainingLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's train one more model using Random Forests\n",
    "# Task 1: make changes here\n",
    "forest_reg = RandomForestRegressor(n_estimators=40, random_state=42)\n",
    "\n",
    "# Task 2: Is everything ok here?\n",
    "display_scores(-cross_val_score(forest_reg, trainingCols, trainingLabels, cv=10, scoring=\"neg_mean_absolute_error\"))\n",
    "display_scores(np.sqrt(-cross_val_score(forest_reg, trainingCols, trainingLabels, cv=10, scoring=\"neg_mean_squared_error\")))\n",
    "train_set_freg = train_set.copy()\n",
    "train_set_freg['predictedCounts'] = cross_val_predict(forest_reg, train_set[['xformWorkHr','temp','dayCount']], trainingLabels, cv=10)\n",
    "train_set_freg['resids'] = train_set_freg['predictedCounts'] - trainingLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Visualizing prediction versus actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Just to plot, try to add more details to the plots\n",
    "\n",
    "times = [8,17]\n",
    "for time in times:\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    fig.clf()\n",
    "    ax = fig.gca()\n",
    "    train_set_freg_time = train_set_freg[train_set.hr == time]\n",
    "    train_set_freg_time.plot(kind = 'line', x = 'dayCount', y = 'cnt', ax = ax)\n",
    "    train_set_freg_time.plot(kind = 'line', x = 'dayCount', y = 'predictedCounts', ax =ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the model\n",
    "\n",
    "**Task 1:** Assign different combination of max_dept and min_samples_leaf and min_samples_split to param_grid - 'max_depth': [28, 30, 32, 34, 36], 'min_samples_leaf': [5, 10, 15, 12],'min_samples_split': [120, 128, 136]\n",
    "\n",
    "**Task 2:** Calculate the best parameter using GridSearchCV and store it in grid_search. Print the parameters. \n",
    "\t\tfrom sklearn.model_selection import GridSearchCV\n",
    "\n",
    "**Task 3:** Fit the training dataset to the calculated best parameter model using the fit() method.\n",
    "\n",
    "**Task 4:** Complete the code to calculate the importance score for each of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Task 1\n",
    "param_grid = [\n",
    "    {'max_depth': [28, 30, 32, 34, 36], 'min_samples_leaf': [5, 10, 15, 12],'min_samples_split': [120, 128, 136]},\n",
    "]\n",
    "\n",
    "# Task 2\n",
    "grid_search = GridSearchCV(rfc_clf, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Task 3\n",
    "grid_search.fit(trainingCols, trainingLabels)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# See the importance score of each attribute in GridSearchCV\n",
    "# Task 4\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ignore\n",
    "#extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"]\n",
    "#cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "#attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "#sorted(zip(feature_importances, attributes), reverse=True)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on test dataset\n",
    "\n",
    "**Task 1:** Extract the relevant data from test_set and store it in X_test\n",
    "\t\t\n",
    "**Task 2:** Extract the relevant data from test_set and store it in y_test\n",
    "\n",
    "**Task 3:** Calculate the predicted values from the model and store it in 'predictedCounts_test' \n",
    "\n",
    "**Task 4:** Calculate the mean squared error using mean_squared_error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "test_set.sort_values('dayCount', axis= 0, inplace=True)\n",
    "\n",
    "# Task 1: Complete the statement below to extract relevant columns from the test dataset\n",
    "test_x_cols = ['xformWorkHr', 'temp','dayCount']\n",
    "\n",
    "# Task 2: Complete the statement below to extract relevant label from test set\n",
    "test_y_cols = 'cnt'\n",
    "\n",
    "X_test = test_set.loc[:,test_x_cols]\n",
    "y_test = test_set.loc[:,test_y_cols]\n",
    "\n",
    "# Task 3: Calculate the predicted values from the model and store it in 'predictedCounts_test'\n",
    "test_set.loc[:,'predictedCounts_test'] = final_model.predict(X_test)\n",
    "\n",
    "# Task 4: Calculate the mean squared error using mean_squared_error function.\n",
    "final_mse = mean_squared_error(y_test, test_set.loc[:,'predictedCounts_test'])\n",
    "print(np.sqrt(final_mse))\n",
    "test_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times = [9,18]\n",
    "for time in times:\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    fig.clf()\n",
    "    ax = fig.gca()\n",
    "    test_set_freg_time = test_set[test_set.hr == time]\n",
    "    test_set_freg_time.plot(kind = 'line', x = 'dayCount', y = 'cnt', ax = ax)\n",
    "    test_set_freg_time.plot(kind = 'line', x = 'dayCount', y = 'predictedCounts_test', ax =ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the residuals\n",
    "Analyzing the difference between the actual and the calculated values\n",
    "\n",
    "Plotting the histogram for the observed difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "## Plot the residuals vs the label, the count of rented bikes.\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.clf()\n",
    "ax = fig.gca()\n",
    "train_set_freg.plot(kind = 'scatter', x = 'cntDeTrended', y = 'resids', alpha = 0.05, color = 'red', ax = ax)\n",
    "plt.xlabel(\"Detrended Count\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(\"Residuals vs Count\")\n",
    "plt.show()\n",
    "## Plotting the resilduals\n",
    "train_set_freg['resids'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_freg.boxplot(column = ['resids'], by = ['hr'], ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.clf()\n",
    "ax = fig.gca()\n",
    "train_set_freg.boxplot(column = ['resids'], by = ['hr'], ax = ax)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Number of bikes')\n",
    "plt.title(lab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.clf()\n",
    "ax = fig.gca()\n",
    "train_set_freg.boxplot(column = ['resids'], by = ['xformWorkHr'], ax = ax)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Number of bikes')\n",
    "plt.title(lab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columnsToDrop = ['instant','casual','registered','atemp','dteday']\n",
    "bikesData = bikesData.drop(columnsToDrop,1)\n",
    "\n",
    "bikesData['isWorking'] = np.where(np.logical_and(bikesData.workingday==1,bikesData.holiday==0),1,0)\n",
    "bikesData['monthCount'] = mnth_cnt(bikesData)\n",
    "bikesData['xformHr'] = np.where(bikesData.hr>4,bikesData.hr-5,bikesData.hr+19)\n",
    "bikesData['dayCount'] = pd.Series(range(bikesData.shape[0]))/24\n",
    "bikesData['xformWorkHr'] = bikesData.isWorking*24 + bikesData.xformHr\n",
    "bikesData = set_day(bikesData)\n",
    "bikesData.describe()\n",
    "\n",
    "columnsToScale = ['temp','hum','windspeed']\n",
    "scaler = StandardScaler()\n",
    "bikesData[columnsToScale] = scaler.fit_transform(bikesData[columnsToScale])\n",
    "arry = bikesData[columnsToScale].as_matrix()\n",
    "bikesData[columnsToScale] = preprocessing.scale(arry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        isWorking = np.where(np.logical_and(X.loc[:,'workingday']==1,X.loc[:,'holiday']==0),1,0)\n",
    "        xformHr = np.where(X.loc[:,'hr']>4,X.loc[:,'hr']-5,X.loc[:,'hr']+19)\n",
    "        xformWorkHr = isWorking*24 + xformHr\n",
    "        return np.c_[X, isWorking, xformHr, xformWorkHr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attr_adder = CombinedAttributesAdder()\n",
    "bikesData1 = attr_adder.transform(bikesData)\n",
    "bikesData = pd.DataFrame(bikesData1, columns=list(bikesData.columns)+[\"isWorking\", \"xformHr\", \"xformWorkHr\"])\n",
    "bikesData.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
